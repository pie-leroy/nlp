{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import, print_function\n",
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "import smart_open, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# fichier incltu dans le projet\n",
    "import save_notebook\n",
    "import utils\n",
    "\n",
    "import time\n",
    "from six.moves import urllib, xrange\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "import collections, math, os, random, zipfile\n",
    "import glob\n",
    "import pickle\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if tensorboard use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16066888981508249581\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1461770649\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 936764343689428296\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model_name = \"word2vec - SKIP GRAM\"\n",
    "files =  os.listdir(\"../wikipedia/data\")\n",
    "now = str(datetime.datetime.now()).replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a unique file big with one wikipedia page per line\n",
    "path=\"../wikipedia/data/\"\n",
    "if not os.path.exists('./data/wikipedia_informatic.txt'):\n",
    "    with open('./data/wikipedia_informatic.txt', 'w+',encoding=\"utf8\" ) as out_file:\n",
    "        for file in files:\n",
    "            if \"ipynb_checkpoints\" in file:\n",
    "                continue\n",
    "            try:\n",
    "                with open(path + file, encoding=\"utf8\") as in_file:\n",
    "                    out_file.write(in_file.read().replace(\"\\n\",\"\"))\n",
    "            except:\n",
    "                continue\n",
    "else:\n",
    "    # We tokenize the file\n",
    "    with open('./data/wikipedia_informatic.txt', 'r', encoding=\"utf8\") as f:\n",
    "        wiki_vocab = f.readlines()\n",
    "    wiki_vocab = [x.strip() for x in wiki_vocab] \n",
    "    wiki_vocab_tokenized = []\n",
    "    wiki_vocab_tokenized = gensim.utils.simple_preprocess(str(wiki_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source https://github.com/udacity/deep-learning/blob/master/embeddings/Skip-Grams-Solution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words (+UNK): [('UNK', 0), ('de', 531786), ('la', 237117), ('le', 206228), ('et', 195066), ('des', 179177), ('les', 174592), ('en', 173191), ('un', 151604), ('est', 133054)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 1000000\n",
    "#https://medium.com/deep-math-machine-learning-ai/chapter-9-2-nlp-code-for-word2vec-neural-network-tensorflow-544db99f5334\n",
    "\n",
    "def build_dataset(vocab_tokenized):\n",
    "    dictionnary_count = {}\n",
    "    dictionnary_count['UNK'] = -1\n",
    "    dictionnary_count.update(collections.Counter(vocab_tokenized).most_common(vocabulary_size - 1))\n",
    "\n",
    "    dictionary = {}\n",
    "    for word in dictionnary_count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "#     data_index = []\n",
    "    \n",
    "    unk_count = 0\n",
    "    for word in vocab_tokenized:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "            \n",
    "#         data_index.append(index)\n",
    "        \n",
    "    dictionnary_count[\"UNK\"] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return dictionnary_count, dictionary, reverse_dictionary\n",
    "\n",
    "\n",
    "dictionnary_count, dictionary, reverse_dictionary = build_dataset(wiki_vocab_tokenized)\n",
    "\n",
    "print('most common words (+UNK):', utils.take(10, dictionnary_count.items()))\n",
    "# print('sample data:', data_index[:10], [reverse_dictionary[i] for i in data_index[:10]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling\n",
    "\n",
    "\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsampling data\n",
    "def subsampling_data(vocab_tokenized, reverse_dictionary, dictionary_count, threshold):\n",
    "    np.random.seed(123)\n",
    "    \n",
    "#     training_dictionary_subsample = dictionary_subsample\n",
    "    training_dictionary_subsample = []\n",
    "    size_of_corpus = len(vocab_tokenized)\n",
    "    \n",
    "    for index in reverse_dictionary:\n",
    "        word = reverse_dictionary[index]\n",
    "        if word == \"UNK\":\n",
    "#             dictionary_subsample[word] = -1\n",
    "            continue\n",
    "            \n",
    "        count_of_word = dictionnary_count[word]\n",
    "        frequence_of_word = count_of_word / size_of_corpus\n",
    "        probability = 1 - ( np.sqrt(threshold / frequence_of_word))\n",
    "        \n",
    "        if probability < np.random.random():\n",
    "            training_dictionary_subsample.append(index)\n",
    "            \n",
    "    return training_dictionary_subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainded_data = subsampling_data(wiki_vocab_tokenized, reverse_dictionary ,dictionnary_count, threshold= 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, index, window_size=5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = index - R if (index - R) > 0 else 0\n",
    "    stop = index + R\n",
    "    target_words = set(words[start:index] + words[index+1:stop+1])\n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders():\n",
    "    x = tf.placeholder(tf.int64, [None], name=\"x\")\n",
    "    y = tf.placeholder(tf.int64, [None, None], name=\"y\")\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for index in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        step_batch = index+batch_size\n",
    "        batch = words[index:step_batch]\n",
    "        for i in range(len(batch)):\n",
    "            batch_x = batch[i]\n",
    "            batch_y = get_target(batch, i, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(x, embedding_size, vocabulary_size):\n",
    "    embedding_matrice = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1, 1), name=\"embedding\")  \n",
    "    embed = tf.nn.embedding_lookup(embedding_matrice, x) \n",
    "\n",
    "    return embedding_matrice, embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(vocab_size, embed, y, embedding_size, n_sampled = 100):\n",
    "# Number of negative y to sample\n",
    "\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((vocab_size, embedding_size), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      y, embed,\n",
    "                                      n_sampled, vocab_size)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    return loss, cost, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(embedding):\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    return normalized_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(normalized_embedding, id_word):   \n",
    "\n",
    "            ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 10 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    id_word = tf.constant(id_word)\n",
    "    \n",
    "#     np_valid_examples = np.array(id_word , valid_size//2)\n",
    "    \n",
    "#     np_valid_examples = np.append(np_valid_examples, random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    #valid_examples = tf.convert_to_tensor(np_valid_examples)\n",
    "    #We use the cosine distance:\n",
    "    valid_dataset = tf.Variable([id_word , valid_size//2])\n",
    "#     valid_dataset = tf.constant(np_valid_examples)\n",
    "    \n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))\n",
    "    return similarity, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_early_stopping(array_average_loss, max_steps_without_decrease):\n",
    "    array_last_average_loss = array_average_loss[-max_steps_without_decrease:len(array_average_loss)]\n",
    "    for element in range(0, max_steps_without_decrease-1):\n",
    "        # if one loss at step t is bigger than loss as t+1 so it is converging\n",
    "        if array_last_average_loss[element] > array_last_average_loss[element +1]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_initializer(x, embedding_size, vocabulary_size, id_word):\n",
    "\n",
    "    train_graph = tf.get_default_graph()\n",
    "\n",
    "    #     on declare \n",
    "    with train_graph.as_default():\n",
    "        x, y = create_placeholders()\n",
    "        embedding, x_embed = get_embed(x, embedding_size, vocabulary_size)\n",
    "        loss, cost, optimizer = negative_sampling(vocabulary_size, x_embed, y, embedding_size, n_sampled = 100)\n",
    "        normalized_embedding = get_result(embedding)\n",
    "        similarity, valid_examples = print_result(normalized_embedding, id_word)\n",
    "        tf.summary.scalar(\"minibatch_cost\", cost)\n",
    "        merge = tf.summary.merge_all()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "    return train_graph, x, y, x_embed, cost, optimizer, normalized_embedding, similarity, valid_examples, merge, saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, reverse_dictionary, epochs, batch_size, window_size, embedding_size, LOG_DIR, id_word, max_steps_without_decrease, restore_last_session=False):  \n",
    "    # initialization of graph\n",
    "    train_graph, x, y, x_embed, cost, optimizer, normalized_embedding, similarity, valid_examples, merge, saver = model_initializer(x_train, embedding_size, len(x_train), id_word)\n",
    " \n",
    "    #init var\n",
    "    date_started = time.time()    \n",
    "    epoch_saver = tf.train.Saver(max_to_keep=3)  # keep 3 last iterations\n",
    "    epoch_initializer = 0\n",
    "    minibatch_cost_sum = 0\n",
    "    \n",
    "    #we store all result for early_stopping\n",
    "    array_average_loss = []\n",
    "   \n",
    "    with tf.Session(graph=train_graph) as sess: \n",
    "\n",
    "        iteration = 1        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        date_started = time.time() \n",
    "        \n",
    "         #If true we load the last checkpoint if exist\n",
    "        if restore_last_session:\n",
    "            try:\n",
    "                    FIND_CHKP = False\n",
    "                    LOG_DIR_SUBFOLDER = LOG_DIR.rsplit('/', 1)[0]+\"/\"\n",
    "                    LOG_DIR_LIST = sorted(glob.glob(os.path.join(LOG_DIR_SUBFOLDER, '*/')), key=os.path.getmtime, reverse=True)\n",
    "                    for folder in LOG_DIR_LIST:\n",
    "                        if os.path.exists(folder + \"save_model\"): \n",
    "                            ckpt = tf.train.get_checkpoint_state(folder +\"/save_model/\")\n",
    "                            \n",
    "                            #check if the checkpoint is the same architecture\n",
    "                            if str(embedding_size) + \"-epoch_saver\" in str(ckpt):\n",
    "                                LOG_DIR = folder\n",
    "                                FIND_CHKP = True\n",
    "                                break\n",
    "                    if FIND_CHKP is False:                \n",
    "                        sys.exit(\"no checkpoint to load\")\n",
    "                    iteration = int(str(ckpt).rsplit(\"epoch_saver-i\")[1].rsplit(\"i-\")[0])\n",
    "                    print(\"checkpoint load from directory \" + LOG_DIR + \" with architecture of \" + str(embedding_size) + \". Restart from iterations : \" + str(iteration))\n",
    "                    \n",
    "                    epoch_saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    epoch_initializer = int(ckpt.model_checkpoint_path.rsplit('i--',1)[1])\n",
    "                    epochs = epochs + epoch_initializer \n",
    "                    \n",
    "                    with open (LOG_DIR +\"/save_model/array_average_loss.save\", 'rb') as file:\n",
    "                        array_average_loss = pickle.load(file)\n",
    "                \n",
    "            except Exception as e: print(str(\" can't load checkpoint => \" + str(e)))\n",
    "\n",
    "                                \n",
    "        #for tensorboard graph\n",
    "        writer = tf.summary.FileWriter(LOG_DIR, sess.graph, filename_suffix=str(date_started)+\"-\")\n",
    "        \n",
    "        for e in range(epoch_initializer, epochs+1):          \n",
    "                \n",
    "            print(\"epoque ============================================================================================================> : \" + str(e) + \" sur \" + str(epochs))  \n",
    "            \n",
    "            n_batches = len(x_train)//batch_size\n",
    "            minibatch_data = get_batches(x_train, batch_size, window_size)\n",
    "            \n",
    "            minibatch_iteration = 0 \n",
    "            for minibatch_X, minibatch_Y in minibatch_data:\n",
    "                \n",
    "                minibatch_cost, _, merge_minibatch= sess.run([cost, optimizer, merge], feed_dict={x: minibatch_X, y: np.array(minibatch_Y)[:, None]})\n",
    "                \n",
    "                #compute loss for all minibatch\n",
    "                minibatch_cost_sum = minibatch_cost_sum + minibatch_cost\n",
    "                average_loss = minibatch_cost_sum / iteration \n",
    "                array_average_loss.append(average_loss)\n",
    "                average_loss = tf.Summary(value=[tf.Summary.Value(tag=\"average_loss\", simple_value=average_loss)])               \n",
    "                writer.add_summary(average_loss, iteration)\n",
    "                    \n",
    "\n",
    "                #we stop learning it the loss does not decrease    \n",
    "                if iteration > max_steps_without_decrease:  \n",
    "                    stop = is_early_stopping(array_average_loss, max_steps_without_decrease)\n",
    "                    if stop:\n",
    "                        print(\"stop because early stopping\")\n",
    "                        break\n",
    "                   \n",
    "                percent_in_epoch =  minibatch_iteration * 100 / utils.round_down(n_batches)\n",
    "\n",
    "                if percent_in_epoch  % 25 == 0:\n",
    "                    print(\"average loss  : \"  + str(average_loss))\n",
    "                    print(\"iteration => \" + str(iteration) + \" and \" + str(percent_in_epoch) + \" % for this epoch at time \" + str(datetime.datetime.now()).replace(\" \",\"\"))                                \n",
    "                    sim, v_examples = sess.run([similarity,valid_examples], feed_dict = {x: np.array(trainded_data)})\n",
    "                        \n",
    "\n",
    "                    valid_word = reverse_dictionary[v_examples[0]]\n",
    "                    top_k = 5 # number of nearest neighbors\n",
    "                    nearest = (-sim[0, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "\n",
    "                    print(\"-------------------------------------------------------------------------------\")\n",
    "                iteration += 1\n",
    "                minibatch_iteration += + 1   \n",
    "                \n",
    "                \n",
    "            #we save model at each epoch  \n",
    "            if not os.path.exists(LOG_DIR +\"/save_model/\"):\n",
    "                os.makedirs(LOG_DIR +\"/save_model\")\n",
    "            epoch_saver.save(sess,  LOG_DIR +\"/save_model/\" + str(embedding_size) + \"-epoch_saver-i\" + str(iteration) + \"i-\", global_step=e)\n",
    "            \n",
    "            with open(LOG_DIR +\"/save_model/array_average_loss.save\", 'wb+') as file:\n",
    "                pickle.dump(array_average_loss, file)\n",
    "        \n",
    "            \n",
    "        save_path = saver.save(sess,os.path.join(LOG_DIR ,'model'+ str(embedding_size) + '.ckpt') )\n",
    "        embed_mat = sess.run(normalized_embedding, feed_dict={x: np.array(trainded_data)})\n",
    "        \n",
    "        return embed_mat, epochs, LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = utils.get_model_name(word_embedding_model_name)\n",
    "LOG_DIR  = './tensorboard-graph/'+ model_name\n",
    "tf.reset_default_graph()\n",
    "date_before_learning = datetime.datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 09:49:49,162 : WARNING : From D:\\Outil\\Anaconda\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "2019-03-15 09:49:53,031 : INFO : Restoring parameters from ./tensorboard-graph/word2vec - SKIP GRAM\\03-14-23-01\\/save_model/150-epoch_saver-i192254i--16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint load from directory ./tensorboard-graph/word2vec - SKIP GRAM\\03-14-23-01\\ with architecture of 150. Restart from iterations : 192254\n",
      "epoque ============================================================================================================> : 16 sur 46\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 7.054694833641406e-06\n",
      "}\n",
      "\n",
      "iteration => 192254 and 0.0 % for this epoch at time 2019-03-1509:49:54.723174\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.00013730395585298538\n",
      "}\n",
      "\n",
      "iteration => 195079 and 25.0 % for this epoch at time 2019-03-1509:53:09.853522\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.00015639953198842704\n",
      "}\n",
      "\n",
      "iteration => 197904 and 50.0 % for this epoch at time 2019-03-1509:58:18.833509\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.000164489058079198\n",
      "}\n",
      "\n",
      "iteration => 200729 and 75.0 % for this epoch at time 2019-03-1510:04:23.276939\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.002111460780724883\n",
      "}\n",
      "\n",
      "iteration => 203554 and 100.0 % for this epoch at time 2019-03-1510:11:05.160705\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "epoque ============================================================================================================> : 17 sur 46\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.0021987799555063248\n",
      "}\n",
      "\n",
      "iteration => 203563 and 0.0 % for this epoch at time 2019-03-1510:11:15.831442\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.002282782457768917\n",
      "}\n",
      "\n",
      "iteration => 206388 and 25.0 % for this epoch at time 2019-03-1510:20:28.055856\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.00227176072075963\n",
      "}\n",
      "\n",
      "iteration => 209213 and 50.0 % for this epoch at time 2019-03-1510:30:17.574779\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.0022450853139162064\n",
      "}\n",
      "\n",
      "iteration => 212038 and 75.0 % for this epoch at time 2019-03-1510:40:07.248375\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.004050863441079855\n",
      "}\n",
      "\n",
      "iteration => 214863 and 100.0 % for this epoch at time 2019-03-1510:54:03.282003\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "epoque ============================================================================================================> : 18 sur 46\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.0041345758363604546\n",
      "}\n",
      "\n",
      "iteration => 214872 and 0.0 % for this epoch at time 2019-03-1510:54:13.530203\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.0041805654764175415\n",
      "}\n",
      "\n",
      "iteration => 217697 and 25.0 % for this epoch at time 2019-03-1511:03:36.786402\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n",
      "average loss  : value {\n",
      "  tag: \"average_loss\"\n",
      "  simple_value: 0.004146866034716368\n",
      "}\n",
      "\n",
      "iteration => 220522 and 50.0 % for this epoch at time 2019-03-1511:18:08.560845\n",
      "Nearest to microsoft: solves, variableliens, isaak, logicielinter, ls,\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "id_word = dictionary[\"microsoft\"]\n",
    "word_embedding, epochs, LOG_DIR = train(trainded_data, reverse_dictionary, 30, 16, 5, 150, LOG_DIR, id_word, 1500, True)\n",
    "\n",
    "time_training = datetime.datetime.now() - date_before_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index label for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(LOG_DIR, \"metadata.tsv\"), 'w+', encoding=\"utf8\")\n",
    "i = 0\n",
    "for idx in trainded_data:\n",
    "    f.write(reverse_dictionary[idx] + '\\n')\n",
    "    if i ==  99999:\n",
    "        break\n",
    "    i = i + 1\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.get_default_graph()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "        \n",
    "    #prepare for embedding on tensorboard\n",
    "    embedding_writer = tf.summary.FileWriter(LOG_DIR +'/projector', sess.graph)\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = 'embedding'\n",
    "    embedding_conf.metadata_path = os.path.join(LOG_DIR+'/projector' ,  'metadata.tsv')\n",
    "    projector.visualize_embeddings(embedding_writer, config)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_notebook_exported = save_notebook.save_notebook(\"word2vec_with_tf.ipynb\")\n",
    "utils.write_result(wiki_vocab_tokenized, word_embedding_model_name, time_training, name_notebook_exported, epochs, \"results.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
